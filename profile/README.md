# QiMeng Project Series â€“ Automated Design with Large Language Models

QiMeng is a research project series dedicated to **fully automated system design using large language models (LLMs)**, covering CPU architecture design, HDL generation, operating system optimization, high-performance kernel generation, compiler construction, and cross-platform program translation.

This organization hosts official implementations and resources for QiMeng-related research.

---

## ğŸ”¹ 1. Automated Chip Design

### ğŸ§  QiMeng-CPU-v1  
**QiMeng-CPU-v1: Automated CPU Design by Learning from Input-Output Examples**  
*Shuyao Cheng et al.*  
**IJCAI 2024**

The first industrial-scale AI-designed RISC-V CPU tape-out. Designed within **5 hours**, over **1700Ã— larger** than prior automated circuits. Successfully booted Linux and achieved performance comparable to **Intel 80486SX**.

- ğŸ“ Repository: [QiMeng-CPU-v1](https://github.com/Qimeng-IPRC/QiMeng-CPU-v1)
- ğŸ“„ Paper (IJCAI 2024): [https://www.ijcai.org/proceedings/2024/425](https://www.ijcai.org/proceedings/2024/425)
- ğŸ“ arXiv: [https://arxiv.org/abs/2501.01234](https://arxiv.org/abs/2306.12456)
  
### ğŸ§  QiMeng-CPU-v2  
**QiMeng-CPU-v2: Automated Superscalar Processor Design by Learning Data Dependencies**  
*Shuyao Cheng, Rui Zhang, Wenkai He, Pengwei Jin, Chongxiao Li, Zidong Du, Xing Hu, Yifan Hao, Guanglin Xu, Yuanbo Wen, Ling Li, Qi Guo, Yunji Chen*  
**IJCAI 2025**

QiMeng-CPU-v2 is the worldâ€™s first AI-designed superscalar CPU, achieving **~380Ã— performance improvement** over prior automated design approaches and reaching competitiveness with human-designed processors such as **ARM Cortex-A53**.

- ğŸ“ Repository: [QiMeng-CPU-v2](https://github.com/Qimeng-IPRC/QiMeng-CPU-v2)
- ğŸ“„ Paper (IJCAI 2025): [https://dl.acm.org/doi/10.24963/ijcai.2025/549](https://dl.acm.org/doi/10.24963/ijcai.2025/549)
- ğŸ“ arXiv: [https://arxiv.org/abs/2501.01234](https://arxiv.org/abs/2501.01234)
---

## ğŸ”¹ 2. Automatic HDL Generation

### âš™ï¸ QiMeng-SALV  
**Signal-Aware Learning for Verilog Code Generation**  
**NeurIPS 2025**

Introduces **signal-level reinforcement learning optimization** for Verilog generation via AST-aware verification, enabling extraction of functionally correct submodules from partially incorrect code.

- ğŸ“ Repository: `QiMeng-SALV`
- ğŸ“„ Paper: NeurIPS 2025  
- ğŸ¤— Hugging Face Model Available

---

### âš™ï¸ QiMeng-CodeV-R1  
**Reasoning-Enhanced Verilog Generation**  
**NeurIPS 2025**

Introduces explicit **reasoning-then-generation** pipeline. Demonstrates **test-time scaling (TTS)** behavior. The 7B model rivals or surpasses large commercial LLMs.

- ğŸ“ Repository: `QiMeng-CodeV-R1`
- ğŸ“„ Paper: NeurIPS 2025  
- ğŸ¤— Hugging Face Model Available

---

### âš™ï¸ QiMeng-CodeV  
**Multi-Level Summarization for HDL Generation**  
**TCAD 2025**

Fine-tuned Verilog-specific LLM for accurate HDL generation. Supports both **Verilog and Chisel** (CodeV-All).

- ğŸ“ Repository: `QiMeng-CodeV`
- ğŸ“„ Paper: TCAD 2025  
- ğŸ¤— Hugging Face Model Available

---

## ğŸ”¹ 3. Automated Software Design

### ğŸ–¥ï¸ AutoOS  
**Make Your OS More Powerful by Exploiting Large Language Models**  
**ICML 2024**

Automatically optimizes Linux kernel configurations for specific hardware platforms using LLMs, targeting **AIoT and embedded systems**.

- ğŸ“ Repository: `AutoOS`
- ğŸ“„ Paper: ICML 2024

---

## ğŸ”¹ 4. High-Performance Library Generation

### âš¡ QiMeng-Attention  
**SOTA Attention Operator Generated by SOTA Attention Algorithm**  
**ACL 2025**

LLM-driven attention kernel generation using **LLM-Thinking-Language (LLM-TL)** with two-stage reasoning workflows. Achieves **up to 35Ã— speedup**.

- ğŸ“ Repository: `QiMeng-Attention`
- ğŸ“„ Paper: ACL 2025

---

### âš¡ QiMeng-TensorOp  
**One-Prompt Tensor Operator Generation**  
**IJCAI 2025**

- 251% of OpenBLAS performance on RISC-V  
- 124% of cuBLAS on NVIDIA GPU

- ğŸ“ Repository: `QiMeng-TensorOp`
- ğŸ“„ Paper: IJCAI 2025

---

### âš¡ QiMeng-GEMM  
**Automated High-Performance GEMM via LLMs**  
**AAAI 2025**

LLM-driven meta-prompting framework for adaptive matrix multiplication optimization across platforms.

- ğŸ“ Repository: `QiMeng-GEMM`
- ğŸ“„ Paper: AAAI 2025

---

## ğŸ”¹ 5. Automated Compiler Generation

### ğŸ§¬ QiMeng-NeuComBack  
**Self-Evolving Neural Compilation**  
**NeurIPS 2025**

Neural compiler framework with iterative **self-debugging and prompt evolution**.

- ğŸ“ Repository: `QiMeng-NeuComBack`
- ğŸ“„ Paper: NeurIPS 2025

---

### ğŸ§¬ VEGA  
**Automated Compiler Backend Generation using Transformers**  
**CGO 2025**

Template-based compiler backend generation via pre-trained transformers.

- ğŸ“„ Paper: CGO 2025

---

### ğŸ§¬ ComBack Dataset  
**Benchmark for Compiler Backend Development**  
**NeurIPS 2024**

- ğŸ“ Dataset Repository: `ComBack`
- ğŸ“„ Paper: NeurIPS 2024

---

## ğŸ”¹ 6. Tensor Program Transcompilation

### ğŸ” QiMeng-MuPa  
**Mutual-Supervised Sequential-to-Parallel Translation**  
**NeurIPS 2025**

First LLM specialized for **automatic HPC code parallelization**.

- ğŸ“ Repository: `QiMeng-MuPa`
- ğŸ“„ Paper: NeurIPS 2025

---

### ğŸ” QiMeng-Xpiler  
**Neural-Symbolic Tensor Transcompiler**  
**OSDI 2025**

- Up to **95% accuracy**
- 2Ã— performance over vendor kernels

- ğŸ“ Repository: `QiMeng-Xpiler`
- ğŸ“„ Paper: OSDI 2025

---

### ğŸ” BabelTower  
**Learning-Based Auto-Parallelized Program Translation**

- Up to **347Ã— speedup**
- Automatic C â†’ CUDA translation

- ğŸ“ Repository: `BabelTower`

---

## ğŸ“Œ License & Contact

All projects follow their individual open-source licenses as specified in each repository.  
For collaboration and research partnership, please contact the maintainers via GitHub Issues.

